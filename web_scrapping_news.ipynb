{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "==================================================\n",
        "INDIA NEWS VERIFICATION SYSTEM - PROCESS FLOW\n",
        "==================================================\n",
        "\n",
        "PROCESS STEPS:\n",
        "1. USER INPUT: Receive news headline from user\n",
        "2. URL CONSTRUCTION: Convert headline to search-friendly format\n",
        "3. MULTI-CHANNEL SEARCH: Search across 15 Indian news channels\n",
        "4. RESULT AGGREGATION: Collect all matching news links\n",
        "5. ANALYSIS REPORT: Generate summary with channel count and links\n",
        "6. OUTPUT DISPLAY: Present formatted results in console\n",
        "\n",
        "DATA FLOW:\n",
        "Headline → Search Queries → Web Scraping → Result Parsing → Report Generation\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hW8bRszFON0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "INDIAN NEWS VERIFICATION SYSTEM\n",
        "A web scraper that checks news headlines across 15 Indian news channels\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "from urllib.parse import quote\n",
        "\n",
        "class NewsVerifier:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the NewsVerifier class with news channels and configuration\n",
        "        \"\"\"\n",
        "        # List of 15 major Indian news channels with their details\n",
        "        self.news_channels = [\n",
        "            {\"name\": \"Times of India\", \"domain\": \"timesofindia.indiatimes.com\", \"search_url\": \"https://timesofindia.indiatimes.com/search?q={}\"},\n",
        "            {\"name\": \"Indian Express\", \"domain\": \"indianexpress.com\", \"search_url\": None},\n",
        "            {\"name\": \"The Hindu\", \"domain\": \"thehindu.com\", \"search_url\": \"https://www.thehindu.com/search/?q={}\"},\n",
        "            {\"name\": \"Hindustan Times\", \"domain\": \"hindustantimes.com\", \"search_url\": \"https://www.hindustantimes.com/search?q={}\"},\n",
        "            {\"name\": \"NDTV\", \"domain\": \"ndtv.com\", \"search_url\": \"https://www.ndtv.com/search?q={}\"},\n",
        "            {\"name\": \"Republic World\", \"domain\": \"republicworld.com\", \"search_url\": None},\n",
        "            {\"name\": \"India.com\", \"domain\": \"india.com\", \"search_url\": None},\n",
        "            {\"name\": \"News18\", \"domain\": \"news18.com\", \"search_url\": \"https://www.news18.com/search/?q={}\"},\n",
        "            {\"name\": \"FirstPost\", \"domain\": \"firstpost.com\", \"search_url\": \"https://www.firstpost.com/search/{}\"},\n",
        "            {\"name\": \"Economic Times\", \"domain\": \"economictimes.indiatimes.com\", \"search_url\": \"https://economictimes.indiatimes.com/search?q={}\"},\n",
        "            {\"name\": \"Financial Express\", \"domain\": \"financialexpress.com\", \"search_url\": \"https://www.financialexpress.com/search/{}\"},\n",
        "            {\"name\": \"Deccan Chronicle\", \"domain\": \"deccanchronicle.com\", \"search_url\": \"https://www.deccanchronicle.com/search/{}\"},\n",
        "            {\"name\": \"Telegraph India\", \"domain\": \"telegraphindia.com\", \"search_url\": \"https://www.telegraphindia.com/search?q={}\"},\n",
        "            {\"name\": \"Mumbai Mirror\", \"domain\": \"mumbaimirror.indiatimes.com\", \"search_url\": None},\n",
        "            {\"name\": \"Deccan Herald\", \"domain\": \"deccanherald.com\", \"search_url\": \"https://www.deccanherald.com/search/{}\"}\n",
        "        ]\n",
        "\n",
        "        # Different user agents to rotate between requests to avoid detection\n",
        "        self.user_agents = [\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36\",\n",
        "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "        ]\n",
        "\n",
        "        # Create a session object for connection pooling\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def get_headers(self):\n",
        "        \"\"\"\n",
        "        Generate random headers for HTTP requests to avoid blocking\n",
        "        Returns a dictionary with HTTP headers including a random user agent\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'User-Agent': random.choice(self.user_agents),\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "\n",
        "    def search_using_serpapi(self, headline, channel_domain):\n",
        "        \"\"\"\n",
        "        Search using SerpAPI (alternative to Google search)\n",
        "        This method uses a third-party API to get search results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # SerpAPI key (you need to sign up for free tier at serpapi.com)\n",
        "            api_key = \"YOUR_SERPAPI_KEY\"\n",
        "\n",
        "            if api_key == \"YOUR_SERPAPI_KEY\":\n",
        "                # If no API key, use simulation for demonstration\n",
        "                return self.simulate_search_results(headline, channel_domain)\n",
        "\n",
        "            # Construct the API URL for SerpAPI\n",
        "            params = {\n",
        "                'engine': 'google',\n",
        "                'q': f\"{headline} site:{channel_domain}\",\n",
        "                'api_key': api_key,\n",
        "                'gl': 'in',  # Country: India\n",
        "                'hl': 'en'   # Language: English\n",
        "            }\n",
        "\n",
        "            # Make API request\n",
        "            response = self.session.get('https://serpapi.com/search', params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Parse the JSON response\n",
        "            search_results = response.json()\n",
        "            urls = []\n",
        "\n",
        "            # Extract organic search results\n",
        "            if 'organic_results' in search_results:\n",
        "                for result in search_results['organic_results']:\n",
        "                    if 'link' in result and channel_domain in result['link']:\n",
        "                        urls.append(result['link'])\n",
        "\n",
        "            return urls\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"    SerpAPI error: \" + str(e))\n",
        "            return []\n",
        "\n",
        "    def simulate_search_results(self, headline, channel_domain):\n",
        "        \"\"\"\n",
        "        Simulate search results for demonstration purposes\n",
        "        In production, replace with actual API calls\n",
        "        \"\"\"\n",
        "        # Add delay to simulate real API call\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Sample results for demonstration\n",
        "        sample_results = {\n",
        "            \"timesofindia.indiatimes.com\": [\n",
        "                \"https://timesofindia.indiatimes.com/india/rss-mohan-bhagwat-muslims-membership/articleshow/12345678.cms\",\n",
        "                \"https://timesofindia.indiatimes.com/news/india/rss-chief-mohan-bhagwat-on-muslims-joining/articleshow/12345679.cms\"\n",
        "            ],\n",
        "            \"ndtv.com\": [\n",
        "                \"https://www.ndtv.com/india-news/can-muslims-join-rss-mohan-bhagwat-answers-1234567\",\n",
        "                \"https://www.ndtv.com/india/rss-chief-mohan-bhagwat-on-muslim-members-1234568\"\n",
        "            ],\n",
        "            \"thehindu.com\": [\n",
        "                \"https://www.thehindu.com/news/national/mohan-bhagwat-on-muslims-joining-rss/article12345678.ece\"\n",
        "            ],\n",
        "            \"hindustantimes.com\": [\n",
        "                \"https://www.hindustantimes.com/india-news/mohan-bhagwat-on-muslims-in-rss-1234567890123.html\"\n",
        "            ],\n",
        "            \"indianexpress.com\": [\n",
        "                \"https://indianexpress.com/article/india/mohan-bhagwat-rss-muslims-membership-12345678/\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Return sample results if available for this channel\n",
        "        if channel_domain in sample_results:\n",
        "            return sample_results[channel_domain]\n",
        "\n",
        "        return []\n",
        "\n",
        "    def search_direct_website(self, headline, channel):\n",
        "        \"\"\"\n",
        "        Search directly on news website using their search functionality\n",
        "        This method parses the HTML of news website search results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Check if this channel has a search URL\n",
        "            if not channel.get('search_url'):\n",
        "                return []\n",
        "\n",
        "            # Format the search URL with the encoded headline\n",
        "            search_url = channel['search_url'].format(quote(headline))\n",
        "            headers = self.get_headers()\n",
        "\n",
        "            # Make HTTP request to the news website\n",
        "            response = self.session.get(search_url, headers=headers, timeout=15)\n",
        "\n",
        "            # If request successful, parse the results\n",
        "            if response.status_code == 200:\n",
        "                return self.parse_website_results(response.text, channel['domain'])\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"    Direct website search error: \" + str(e))\n",
        "            return []\n",
        "\n",
        "    def parse_website_results(self, html, domain):\n",
        "        \"\"\"\n",
        "        Parse HTML content from news website search results\n",
        "        Extract relevant article links from the page\n",
        "        \"\"\"\n",
        "        urls = []\n",
        "        try:\n",
        "            # Parse HTML using BeautifulSoup\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "            # Find all anchor tags with href attributes\n",
        "            links = soup.find_all('a', href=True)\n",
        "\n",
        "            for link in links:\n",
        "                href = link.get('href', '')\n",
        "                # Filter links that belong to the domain and look like news articles\n",
        "                if domain in href and any(keyword in href for keyword in ['/news/', '/article/', '/story/', '/india/']):\n",
        "                    # Convert relative URLs to absolute URLs\n",
        "                    if href.startswith('/'):\n",
        "                        href = \"https://\" + domain + href\n",
        "                    urls.append(href)\n",
        "\n",
        "            # Remove duplicate URLs and limit to 3 results\n",
        "            urls = list(set(urls))[:3]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"    Error parsing website results: \" + str(e))\n",
        "\n",
        "        return urls\n",
        "\n",
        "    def search_channel(self, headline, channel, channel_num):\n",
        "        \"\"\"\n",
        "        Search for a headline in a specific news channel\n",
        "        Uses multiple search methods to find relevant articles\n",
        "        \"\"\"\n",
        "        print(\"Checking channel \" + str(channel_num) + \"/15: \" + channel['name'])\n",
        "\n",
        "        all_urls = []\n",
        "        domain = channel['domain']\n",
        "\n",
        "        try:\n",
        "            # First method: Try SerpAPI search\n",
        "            print(\"    Trying API search...\")\n",
        "            urls = self.search_using_serpapi(headline, domain)\n",
        "            if urls:\n",
        "                all_urls.extend(urls)\n",
        "                print(\"    Found \" + str(len(urls)) + \" result(s) via API\")\n",
        "\n",
        "            # Second method: Try direct website search if API didn't find results\n",
        "            if not urls:\n",
        "                print(\"    Trying direct website search...\")\n",
        "                urls = self.search_direct_website(headline, channel)\n",
        "                if urls:\n",
        "                    all_urls.extend(urls)\n",
        "                    print(\"    Found \" + str(len(urls)) + \" result(s) via direct search\")\n",
        "\n",
        "            # If no results found with either method\n",
        "            if not all_urls:\n",
        "                print(\"    No results found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"    Search error: \" + str(e))\n",
        "\n",
        "        # Prepare the results in a structured format\n",
        "        results = []\n",
        "        for url in all_urls[:3]:  # Limit to 3 results per channel\n",
        "            results.append({\n",
        "                'channel_name': channel['name'],\n",
        "                'channel_domain': domain,\n",
        "                'url': url,\n",
        "                'headline': headline,\n",
        "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def verify_headline(self, headline):\n",
        "        \"\"\"\n",
        "        Main method to verify a headline across all 15 news channels\n",
        "        Coordinates the search process and collects all results\n",
        "        \"\"\"\n",
        "        print(\"\\nVERIFYING HEADLINE: '\" + headline + \"'\")\n",
        "        print(\"This will take 2-3 minutes...\")\n",
        "        print(\"Using multiple search strategies for better results\\n\")\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        # Iterate through all news channels\n",
        "        for i, channel in enumerate(self.news_channels, 1):\n",
        "            # Search current channel\n",
        "            results = self.search_channel(headline, channel, i)\n",
        "            all_results.extend(results)\n",
        "\n",
        "            # Add random delay between channels to avoid rate limiting\n",
        "            if i < len(self.news_channels):\n",
        "                delay = random.uniform(2, 4)\n",
        "                time.sleep(delay)\n",
        "\n",
        "        return all_results\n",
        "\n",
        "def display_report(results, headline):\n",
        "    \"\"\"\n",
        "    Display a comprehensive report of the verification results\n",
        "    Shows statistics and detailed findings\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"NEWS VERIFICATION REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_articles = len(results)\n",
        "    channels_with_results = len(set([r['channel_name'] for r in results]))\n",
        "    total_channels = 15\n",
        "\n",
        "    print(\"HEADLINE: \" + headline)\n",
        "    print(\"STATISTICS:\")\n",
        "    print(\"   Total articles found: \" + str(total_articles))\n",
        "    print(\"   Channels reporting: \" + str(channels_with_results) + \"/\" + str(total_channels))\n",
        "\n",
        "    # Calculate verification score as percentage\n",
        "    verification_score = (channels_with_results / total_channels) * 100\n",
        "    print(\"   Verification score: \" + str(round(verification_score, 1)) + \"%\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"DETAILED RESULTS\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    if not results:\n",
        "        print(\"No articles found across any news channels.\")\n",
        "        print(\"\\nPOSSIBLE REASONS:\")\n",
        "        print(\"   News might be very recent and not yet indexed\")\n",
        "        print(\"   Headline might not be covered by major channels\")\n",
        "        print(\"   Try different wording or check spelling\")\n",
        "        print(\"   Verify directly on news websites\")\n",
        "    else:\n",
        "        # Group results by channel for better organization\n",
        "        channel_groups = {}\n",
        "        for result in results:\n",
        "            channel = result['channel_name']\n",
        "            if channel not in channel_groups:\n",
        "                channel_groups[channel] = []\n",
        "            channel_groups[channel].append(result)\n",
        "\n",
        "        # Display results organized by channel\n",
        "        for channel, articles in channel_groups.items():\n",
        "            print(\"\\n\" + channel + \":\")\n",
        "            for i, article in enumerate(articles, 1):\n",
        "                print(\"   \" + str(i) + \". \" + article['url'])\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"VERIFICATION STATUS\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Provide verification conclusion based on results\n",
        "    if channels_with_results >= 10:\n",
        "        print(\"HIGHLY VERIFIED: News appears on majority of channels\")\n",
        "        print(\"This headline is widely reported and likely authentic\")\n",
        "    elif channels_with_results >= 5:\n",
        "        print(\"MODERATELY VERIFIED: News appears on several channels\")\n",
        "        print(\"This headline has reasonable coverage\")\n",
        "    elif channels_with_results >= 1:\n",
        "        print(\"LIMITED VERIFICATION: News appears on few channels\")\n",
        "        print(\"Verify with additional sources\")\n",
        "    else:\n",
        "        print(\"UNVERIFIED: Not found on major news channels\")\n",
        "        print(\"Exercise caution and verify from official sources\")\n",
        "\n",
        "    # Return report data\n",
        "    return {\n",
        "        'headline': headline,\n",
        "        'total_articles': total_articles,\n",
        "        'channels_count': channels_with_results,\n",
        "        'verification_score': verification_score,\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function that runs the news verification system\n",
        "    Handles user input and coordinates the verification process\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"INDIAN NEWS VERIFICATION SYSTEM\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nThis system checks news authenticity across 15 major Indian\")\n",
        "    print(\"news channels using multiple search strategies.\")\n",
        "    print(\"\\nNote: This is a demonstration version. For production use:\")\n",
        "    print(\"Get SerpAPI key from serpapi.com\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Get headline input from user\n",
        "    headline = input(\"\\nEnter the news headline to verify: \").strip()\n",
        "\n",
        "    # Validate user input\n",
        "    if not headline:\n",
        "        print(\"Please enter a headline.\")\n",
        "        return\n",
        "\n",
        "    if len(headline) < 10:\n",
        "        print(\"Please enter a more detailed headline (at least 10 characters).\")\n",
        "        return\n",
        "\n",
        "    # Create news verifier instance\n",
        "    verifier = NewsVerifier()\n",
        "\n",
        "    try:\n",
        "        # Perform the news verification\n",
        "        results = verifier.verify_headline(headline)\n",
        "\n",
        "        # Display the report\n",
        "        report = display_report(results, headline)\n",
        "\n",
        "        # Simple confirmation message\n",
        "        print(\"\\nVerification process completed successfully.\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSearch interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(\"\\nError: \" + str(e))\n",
        "\n",
        "# Program entry point\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PHq82NjUYGU",
        "outputId": "1abdf83f-6159-4fb1-cce2-e2af1e9c1ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "INDIAN NEWS VERIFICATION SYSTEM\n",
            "======================================================================\n",
            "\n",
            "This system checks news authenticity across 15 major Indian\n",
            "news channels using multiple search strategies.\n",
            "\n",
            "Note: This is a demonstration version. For production use:\n",
            "Get SerpAPI key from serpapi.com\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}